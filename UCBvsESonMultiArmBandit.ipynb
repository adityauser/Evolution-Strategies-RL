{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class Enviroment:\n",
    "    def __init__(self, is_bernoulli, no_arms=10):\n",
    "        self.no_arms = no_arms\n",
    "        \n",
    "        self.pull_arm = self.pull_arm_bernaulli if is_bernoulli else self.pull_arm_gaussion\n",
    "            \n",
    "        self.arms = {}\n",
    "        for arm in range(self.no_arms):\n",
    "            self.arms[arm] = np.random.rand()\n",
    "        \n",
    "        self.best_arm = np.argmax([[self.arms[arm] for arm in range(self.no_arms)]])\n",
    "        \n",
    "    def update(self):\n",
    "        for arm in range(self.no_arms):\n",
    "            self.arms[arm] = np.random.rand()\n",
    "        self.best_arm = np.argmax([[self.arms[arm] for arm in range(self.no_arms)]])\n",
    "        \n",
    "    def pull_arm_bernaulli(self, arm):\n",
    "        return 1 if self.arms[arm] > np.random.rand() else 0\n",
    "    \n",
    "    def pull_arm_gaussion(self, arm):\n",
    "        return np.random.normal(self.arms[arm], scale=0.1)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentUCB:\n",
    "    def __init__(self, no_arms=10):\n",
    "        self.no_arms = no_arms\n",
    "        self.info = {}\n",
    "        for arm in range(self.no_arms):\n",
    "            self.info[arm] = {}\n",
    "            self.info[arm]['likely'] = 0\n",
    "            self.info[arm]['no_visited'] = 0\n",
    "            self.info[arm]['is_visited'] = False\n",
    "            self.info[arm]['value'] = 0\n",
    "        self.round = 0\n",
    "        self.score = 0\n",
    "        \n",
    "    def pull(self):\n",
    "        self.round += 1\n",
    "        \n",
    "        for arm in range(self.no_arms):\n",
    "            if not self.info[arm]['is_visited']:\n",
    "                self.info[arm]['is_visited'] = True\n",
    "                self.info[arm]['no_visited'] += 1\n",
    "                return arm\n",
    "            \n",
    "        arm = np.argmax(np.asarray([self.info[arm]['likely'] for arm in range(self.no_arms)]))\n",
    "        self.info[arm]['no_visited'] += 1\n",
    "        \n",
    "        return arm\n",
    "    \n",
    "    def reset(self):\n",
    "        self.info = {}\n",
    "        for arm in range(self.no_arms):\n",
    "            self.info[arm] = {}\n",
    "            self.info[arm]['likely'] = 0\n",
    "            self.info[arm]['no_visited'] = 0\n",
    "            self.info[arm]['is_visited'] = False\n",
    "            self.info[arm]['value'] = 0\n",
    "        self.round = 0    \n",
    "    \n",
    "    def update(self, action, reward):\n",
    "        \n",
    "        self.info[action]['value'] += (reward-self.info[action]['value'])/self.info[action]['no_visited']\n",
    "        \n",
    "        if all([self.info[arm]['is_visited'] for arm in range(self.no_arms)]):\n",
    "            for arm in range(self.no_arms):\n",
    "                self.info[arm]['likely'] = self.info[arm]['value'] + \\\n",
    "                np.sqrt((2*np.log(self.round))/self.info[arm]['no_visited'])\n",
    "                \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "from torch.autograd import Variable\n",
    "\n",
    "import copy\n",
    "import operator\n",
    "\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, num_inputs, action_space,settings={}):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        #should hidden units have biases?\n",
    "        _b = True\n",
    "        hid_layers = 2  #how many hidden layers\n",
    "        sz = 64 #how many neurons per layer\n",
    "\n",
    "        num_outputs = action_space\n",
    "\n",
    "        #overwritable defaults\n",
    "        af = nn.ReLU \n",
    "        oaf = nn.Sigmoid\n",
    "        \n",
    "\n",
    "        afunc = {'relu':nn.ReLU,'tanh':nn.Tanh,'linear':lambda : linear,'sigmoid':nn.Sigmoid}\n",
    "        \n",
    "        self.stddev = 0.1\n",
    "        self._af = af\n",
    "        self.af = af()\n",
    "        self.sigmoid = oaf()\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "\n",
    "        #first fully-connected layer changing from input-size representation to hidden-size representation\n",
    "        self.fc1 = nn.Linear(num_inputs, sz, bias=_b) \n",
    "\n",
    "        self.hidden_layers = []\n",
    "\n",
    "        #create all the hidden layers\n",
    "        for x in range(hid_layers):\n",
    "            self.hidden_layers.append(nn.Linear(sz, sz, bias=True))\n",
    "            self.add_module(\"hl%d\"%x,self.hidden_layers[-1])\n",
    "\n",
    "        #create the hidden -> output layer\n",
    "        self.fc_out = nn.Linear(sz, num_outputs, bias=_b)\n",
    "\n",
    "        self.train()\n",
    "\n",
    "    def forward(self, inputs, intermediate=None, debug=False):\n",
    "\n",
    "        x = inputs\n",
    "        x = torch.cat(x)\n",
    "        #fully connection input -> hidden\n",
    "        x = self.fc1(x.float())\n",
    "            \n",
    "        x = self._af()(x)\n",
    "        \n",
    "        #propagate signal through hidden layers\n",
    "        for idx,layer in enumerate(self.hidden_layers):\n",
    "\n",
    "            #do fc computation\n",
    "            x = layer(x)\n",
    "\n",
    "            #run it through activation function\n",
    "            x = self._af()(x)\n",
    "\n",
    "            if intermediate == idx:\n",
    "                cached = x\n",
    "        \n",
    "        #output layer\n",
    "        x = self.fc_out(x)\n",
    "        #Add noise\n",
    "        x = x+torch.autograd.Variable(torch.randn(x.size()).cpu() * self.stddev)\n",
    "        #softmax\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        if intermediate!=None:\n",
    "            return x,cached\n",
    "\n",
    "        return x\n",
    "    \n",
    "    #function to grab current flattened neural network weights\n",
    "    def extract_parameters(self):\n",
    "        tot_size = self.count_parameters()\n",
    "        pvec = np.zeros(tot_size, np.float32)\n",
    "        count = 0\n",
    "        for param in self.parameters():\n",
    "            sz = param.data.numpy().flatten().shape[0]\n",
    "            pvec[count:count + sz] = param.data.numpy().flatten()\n",
    "            count += sz\n",
    "        return pvec.copy()\n",
    "    \n",
    "    #function to inject a flat vector of ANN parameters into the model's current neural network weights\n",
    "    def inject_parameters(self, pvec):\n",
    "        tot_size = self.count_parameters()\n",
    "        count = 0\n",
    "\n",
    "        for param in self.parameters():\n",
    "            sz = param.cpu().data.numpy().flatten().shape[0]\n",
    "            raw = pvec[count:count + sz]\n",
    "            reshaped = raw.reshape(param.cpu().data.numpy().shape)\n",
    "            param.data = torch.from_numpy(reshaped).float()\n",
    "            count += sz\n",
    "\n",
    "        return pvec\n",
    "\n",
    "    #count how many parameters are in the model\n",
    "    def count_parameters(self):\n",
    "        count = 0\n",
    "        for param in self.parameters():\n",
    "            #print param.data.numpy().shape\n",
    "            count += param.cpu().data.numpy().flatten().shape[0]\n",
    "        return count\n",
    "\n",
    "class Individual:\n",
    "    def __init__(self, no_arms=10, mag=0.1, trials=50):\n",
    "        self.no_arms = no_arms\n",
    "        self.mag = mag\n",
    "        self.trials = trials\n",
    "        self.net_rwd = 0\n",
    "        self.score = 0\n",
    "        self.fitness = 0\n",
    "        self.played = False\n",
    "        self.model = Model(no_arms*2, no_arms)\n",
    "        self.info = {}\n",
    "        for arm in range(self.no_arms):\n",
    "            self.info[arm] = {}\n",
    "            self.info[arm]['no_visited'] = 0\n",
    "            self.info[arm]['value'] = 0\n",
    "        \n",
    "    def pull(self):\n",
    "        visits = torch.Tensor(np.array([self.info[arm]['no_visited'] for arm in range(self.no_arms)]))\n",
    "        values = torch.Tensor(np.array([self.info[arm]['value'] for arm in range(self.no_arms)]))\n",
    "        actions = self.model([visits, values])\n",
    "        try:\n",
    "            arm = np.random.choice(self.no_arms, 1, p=actions.detach().numpy())[0]\n",
    "        except:\n",
    "            raise TypeError(\"check\", actions)\n",
    "        self.info[arm]['no_visited'] += 1\n",
    "        return arm\n",
    "    \n",
    "    def reset(self):\n",
    "        self.info = {}\n",
    "        for arm in range(self.no_arms):\n",
    "            self.info[arm] = {}\n",
    "            self.info[arm]['no_visited'] = 0\n",
    "            self.info[arm]['value'] = 0\n",
    "    \n",
    "    def mutate(self, mu, sigma=0.1, mag=None):\n",
    "        if mag==None:\n",
    "            mag = self.mag\n",
    "        parameters = self.model.extract_parameters()\n",
    "        pertubation = np.array([np.random.normal(loc=mean, scale=mag) for mean in mu])\n",
    "        updated_paramaters = pertubation\n",
    "        child = Individual(self.no_arms, self.mag, self.trials)\n",
    "        child.model.inject_parameters(updated_paramaters.astype(float)) \n",
    "        return child\n",
    "    \n",
    "    def run(self, envs):\n",
    "        if not self.played:\n",
    "            for env in envs:\n",
    "                for _ in range(self.trials):\n",
    "                    arm = self.pull()\n",
    "                    r = env.pull_arm(arm)\n",
    "                    self.info[arm]['value'] += (r-self.info[arm]['value'])/self.info[arm]['no_visited']\n",
    "                    self.net_rwd += r \n",
    "                    self.score += int(arm==env.best_arm)\n",
    "                self.reset()\n",
    "            self.fitness += self.score/(self.trials*len(envs))\n",
    "            self.played = True\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramaters = {\n",
    "    'psize': [10, 25, 50, 100, 200, 500],\n",
    "    'mag': [0.001, 0.005, 0.01, 0.05, 0.07, 0.1, 0.2, 0.5],\n",
    "    'greedy_kill': [1, 3, 5, 10],\n",
    "    'size': [4, 8, 16, 32, 64],\n",
    "    'layers': [1, 2, 3, 4, 5],\n",
    "    'arms': [2, 4, 8, 16, 32, 64],\n",
    "    'marks': [1/5, 1/10, 1/20, 1/50, 1/100]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'psize': [10, 25, 50, 100, 200, 500],\n",
       " 'mag': [0.001, 0.005, 0.01, 0.05, 0.07, 0.1, 0.2, 0.5],\n",
       " 'greedy_kill': [1, 3, 5, 10],\n",
       " 'size': [4, 8, 16, 32, 64],\n",
       " 'layers': [1, 2, 3, 4, 5],\n",
       " 'arms': [2, 4, 8, 16, 32, 64],\n",
       " 'marks': [0.2, 0.1, 0.05, 0.02, 0.01]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paramaters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation:  0\n",
      "pop fitness 0.09931999999999999\n",
      "elite fitness 0.1216\n",
      "Generation:  1\n",
      "pop fitness 0.10935999999999998\n",
      "elite fitness 0.1268\n",
      "Generation:  2\n",
      "pop fitness 0.11484\n",
      "elite fitness 0.134\n",
      "Generation:  3\n",
      "pop fitness 0.11899999999999998\n",
      "elite fitness 0.13840000000000002\n",
      "Generation:  4\n",
      "pop fitness 0.12368\n",
      "elite fitness 0.14200000000000002\n",
      "Generation:  5\n",
      "pop fitness 0.12864\n",
      "elite fitness 0.14520000000000002\n",
      "Generation:  6\n",
      "pop fitness 0.134\n",
      "elite fitness 0.1504\n",
      "Generation:  7\n",
      "pop fitness 0.13724\n",
      "elite fitness 0.152\n",
      "Generation:  8\n",
      "pop fitness 0.14308\n",
      "elite fitness 0.16440000000000002\n",
      "Generation:  9\n",
      "pop fitness 0.15496000000000001\n",
      "elite fitness 0.17959999999999998\n",
      "Generation:  10\n",
      "pop fitness 0.16532\n",
      "elite fitness 0.1944\n",
      "Generation:  11\n",
      "pop fitness 0.17784\n",
      "elite fitness 0.202\n",
      "Generation:  12\n",
      "pop fitness 0.1898\n",
      "elite fitness 0.21880000000000002\n",
      "Generation:  13\n",
      "pop fitness 0.19640000000000002\n",
      "elite fitness 0.2192\n",
      "Generation:  14\n",
      "pop fitness 0.20592\n",
      "elite fitness 0.23440000000000003\n",
      "Generation:  15\n",
      "pop fitness 0.20992\n",
      "elite fitness 0.242\n",
      "Generation:  16\n",
      "pop fitness 0.21388000000000001\n",
      "elite fitness 0.2464\n",
      "Generation:  17\n",
      "pop fitness 0.21396\n",
      "elite fitness 0.2464\n",
      "Generation:  18\n",
      "pop fitness 0.21584\n",
      "elite fitness 0.2492\n",
      "Generation:  19\n",
      "pop fitness 0.21664000000000003\n",
      "elite fitness 0.2492\n",
      "Generation:  20\n",
      "pop fitness 0.21971999999999997\n",
      "elite fitness 0.256\n",
      "Generation:  21\n",
      "pop fitness 0.22715999999999997\n",
      "elite fitness 0.28080000000000005\n",
      "Generation:  22\n",
      "pop fitness 0.23075999999999997\n",
      "elite fitness 0.28200000000000003\n",
      "Generation:  23\n",
      "pop fitness 0.23323999999999998\n",
      "elite fitness 0.28200000000000003\n",
      "Generation:  24\n",
      "pop fitness 0.23596\n",
      "elite fitness 0.28240000000000004\n",
      "Generation:  25\n",
      "pop fitness 0.23683999999999997\n",
      "elite fitness 0.29400000000000004\n",
      "Generation:  26\n",
      "pop fitness 0.24192\n",
      "elite fitness 0.30640000000000006\n",
      "Generation:  27\n",
      "pop fitness 0.24731999999999998\n",
      "elite fitness 0.3124\n",
      "Generation:  28\n",
      "pop fitness 0.24519999999999997\n",
      "elite fitness 0.3124\n",
      "Generation:  29\n",
      "pop fitness 0.25044\n",
      "elite fitness 0.3124\n",
      "Generation:  30\n",
      "pop fitness 0.25487999999999994\n",
      "elite fitness 0.3164\n",
      "Generation:  31\n",
      "pop fitness 0.25492000000000004\n",
      "elite fitness 0.3164\n",
      "Generation:  32\n",
      "pop fitness 0.25567999999999996\n",
      "elite fitness 0.3164\n",
      "Generation:  33\n",
      "pop fitness 0.25348\n",
      "elite fitness 0.3164\n",
      "Generation:  34\n",
      "pop fitness 0.26224000000000003\n",
      "elite fitness 0.3252\n",
      "Generation:  35\n",
      "pop fitness 0.2654\n",
      "elite fitness 0.3252\n",
      "Generation:  36\n",
      "pop fitness 0.27176\n",
      "elite fitness 0.33599999999999997\n",
      "Generation:  37\n",
      "pop fitness 0.27856000000000003\n",
      "elite fitness 0.35679999999999995\n",
      "Generation:  38\n",
      "pop fitness 0.28108\n",
      "elite fitness 0.36199999999999993\n",
      "Generation:  39\n",
      "pop fitness 0.28204\n",
      "elite fitness 0.36199999999999993\n",
      "Generation:  40\n",
      "pop fitness 0.28156\n",
      "elite fitness 0.36199999999999993\n",
      "Generation:  41\n",
      "pop fitness 0.2844\n",
      "elite fitness 0.36479999999999996\n",
      "Generation:  42\n",
      "pop fitness 0.2858\n",
      "elite fitness 0.36639999999999995\n",
      "Generation:  43\n",
      "pop fitness 0.29776\n",
      "elite fitness 0.3736\n",
      "Generation:  44\n",
      "pop fitness 0.29464\n",
      "elite fitness 0.3736\n",
      "Generation:  45\n",
      "pop fitness 0.30219999999999997\n",
      "elite fitness 0.3736\n",
      "Generation:  46\n",
      "pop fitness 0.30108\n",
      "elite fitness 0.3736\n",
      "Generation:  47\n",
      "pop fitness 0.30347999999999997\n",
      "elite fitness 0.37679999999999997\n",
      "Generation:  48\n",
      "pop fitness 0.3054\n",
      "elite fitness 0.37679999999999997\n",
      "Generation:  49\n",
      "pop fitness 0.30896\n",
      "elite fitness 0.37959999999999994\n",
      "Generation:  50\n",
      "pop fitness 0.31648\n",
      "elite fitness 0.3832\n",
      "Generation:  51\n",
      "pop fitness 0.31732\n",
      "elite fitness 0.3832\n",
      "Generation:  52\n",
      "pop fitness 0.31823999999999997\n",
      "elite fitness 0.3832\n",
      "Generation:  53\n",
      "pop fitness 0.31648\n",
      "elite fitness 0.3832\n",
      "Generation:  54\n",
      "pop fitness 0.31792000000000004\n",
      "elite fitness 0.3832\n",
      "Generation:  55\n",
      "pop fitness 0.32195999999999997\n",
      "elite fitness 0.3832\n",
      "Generation:  56\n",
      "pop fitness 0.31928\n",
      "elite fitness 0.3832\n",
      "Generation:  57\n",
      "pop fitness 0.31608\n",
      "elite fitness 0.3832\n",
      "Generation:  58\n",
      "pop fitness 0.32552\n",
      "elite fitness 0.3832\n",
      "Generation:  59\n",
      "pop fitness 0.32144000000000006\n",
      "elite fitness 0.3832\n",
      "Generation:  60\n",
      "pop fitness 0.32904000000000005\n",
      "elite fitness 0.3832\n",
      "Generation:  61\n",
      "pop fitness 0.32448\n",
      "elite fitness 0.3832\n",
      "Generation:  62\n",
      "pop fitness 0.32704\n",
      "elite fitness 0.3832\n",
      "Generation:  63\n",
      "pop fitness 0.31656\n",
      "elite fitness 0.3832\n",
      "Generation:  64\n",
      "pop fitness 0.32372\n",
      "elite fitness 0.3848\n",
      "Generation:  65\n",
      "pop fitness 0.32512\n",
      "elite fitness 0.3848\n",
      "Generation:  66\n",
      "pop fitness 0.32204\n",
      "elite fitness 0.3848\n",
      "Generation:  67\n",
      "pop fitness 0.31771999999999995\n",
      "elite fitness 0.3848\n",
      "Generation:  68\n",
      "pop fitness 0.31956\n",
      "elite fitness 0.3848\n",
      "Generation:  69\n",
      "pop fitness 0.32539999999999997\n",
      "elite fitness 0.3848\n",
      "Generation:  70\n",
      "pop fitness 0.32944\n",
      "elite fitness 0.3848\n",
      "Generation:  71\n",
      "pop fitness 0.32264\n",
      "elite fitness 0.3848\n",
      "Generation:  72\n",
      "pop fitness 0.33496000000000004\n",
      "elite fitness 0.3848\n",
      "Generation:  73\n",
      "pop fitness 0.32896000000000003\n",
      "elite fitness 0.3848\n",
      "Generation:  74\n",
      "pop fitness 0.32472\n",
      "elite fitness 0.3848\n",
      "Generation:  75\n",
      "pop fitness 0.32348\n",
      "elite fitness 0.3848\n",
      "Generation:  76\n",
      "pop fitness 0.3236\n",
      "elite fitness 0.3848\n",
      "Generation:  77\n",
      "pop fitness 0.32304\n",
      "elite fitness 0.3848\n",
      "Generation:  78\n",
      "pop fitness 0.33696000000000004\n",
      "elite fitness 0.3848\n",
      "Generation:  79\n",
      "pop fitness 0.32472\n",
      "elite fitness 0.3848\n",
      "Generation:  80\n",
      "pop fitness 0.32276000000000005\n",
      "elite fitness 0.3848\n",
      "Generation:  81\n",
      "pop fitness 0.33236000000000004\n",
      "elite fitness 0.3848\n",
      "Generation:  82\n",
      "pop fitness 0.33031999999999995\n",
      "elite fitness 0.3848\n",
      "Generation:  83\n",
      "pop fitness 0.33332000000000006\n",
      "elite fitness 0.3848\n",
      "Generation:  84\n",
      "pop fitness 0.33480000000000004\n",
      "elite fitness 0.3864000000000001\n",
      "Generation:  85\n",
      "pop fitness 0.3378\n",
      "elite fitness 0.3864000000000001\n",
      "Generation:  86\n",
      "pop fitness 0.33104\n",
      "elite fitness 0.3864000000000001\n",
      "Generation:  87\n",
      "pop fitness 0.33352000000000004\n",
      "elite fitness 0.3872000000000001\n",
      "Generation:  88\n",
      "pop fitness 0.33372\n",
      "elite fitness 0.3888\n",
      "Generation:  89\n",
      "pop fitness 0.336\n",
      "elite fitness 0.3888\n",
      "Generation:  90\n",
      "pop fitness 0.34075999999999995\n",
      "elite fitness 0.39080000000000004\n",
      "Generation:  91\n",
      "pop fitness 0.34180000000000005\n",
      "elite fitness 0.39080000000000004\n",
      "Generation:  92\n",
      "pop fitness 0.35284\n",
      "elite fitness 0.39080000000000004\n",
      "Generation:  93\n",
      "pop fitness 0.35047999999999996\n",
      "elite fitness 0.39160000000000006\n",
      "Generation:  94\n",
      "pop fitness 0.3522\n",
      "elite fitness 0.3928\n",
      "Generation:  95\n",
      "pop fitness 0.35708\n",
      "elite fitness 0.3928\n",
      "Generation:  96\n",
      "pop fitness 0.345\n",
      "elite fitness 0.3928\n",
      "Generation:  97\n",
      "pop fitness 0.3373199999999999\n",
      "elite fitness 0.3952\n",
      "Generation:  98\n",
      "pop fitness 0.34668\n",
      "elite fitness 0.3952\n",
      "Generation:  99\n",
      "pop fitness 0.34116\n",
      "elite fitness 0.3952\n",
      "Generation:  100\n",
      "pop fitness 0.35192\n",
      "elite fitness 0.396\n",
      "Generation:  101\n",
      "pop fitness 0.34524\n",
      "elite fitness 0.396\n",
      "Generation:  102\n",
      "pop fitness 0.35544000000000003\n",
      "elite fitness 0.396\n",
      "Generation:  103\n",
      "pop fitness 0.35355999999999993\n",
      "elite fitness 0.396\n",
      "Generation:  104\n",
      "pop fitness 0.35668000000000005\n",
      "elite fitness 0.396\n",
      "Generation:  105\n",
      "pop fitness 0.35384\n",
      "elite fitness 0.396\n",
      "Generation:  106\n",
      "pop fitness 0.36123999999999995\n",
      "elite fitness 0.3968\n",
      "Generation:  107\n",
      "pop fitness 0.35272\n",
      "elite fitness 0.398\n",
      "Generation:  108\n",
      "pop fitness 0.35727999999999993\n",
      "elite fitness 0.398\n",
      "Generation:  109\n",
      "pop fitness 0.35092\n",
      "elite fitness 0.398\n",
      "Generation:  110\n",
      "pop fitness 0.35752\n",
      "elite fitness 0.398\n",
      "Generation:  111\n",
      "pop fitness 0.3561599999999999\n",
      "elite fitness 0.398\n",
      "Generation:  112\n",
      "pop fitness 0.35467999999999994\n",
      "elite fitness 0.39880000000000004\n",
      "Generation:  113\n",
      "pop fitness 0.35891999999999996\n",
      "elite fitness 0.39960000000000007\n",
      "Generation:  114\n",
      "pop fitness 0.35088\n",
      "elite fitness 0.39960000000000007\n",
      "Generation:  115\n",
      "pop fitness 0.36056\n",
      "elite fitness 0.39960000000000007\n",
      "Generation:  116\n",
      "pop fitness 0.35136\n",
      "elite fitness 0.39960000000000007\n",
      "Generation:  117\n",
      "pop fitness 0.35648\n",
      "elite fitness 0.4\n",
      "Generation:  118\n",
      "pop fitness 0.35896\n",
      "elite fitness 0.4\n",
      "Generation:  119\n",
      "pop fitness 0.35292\n",
      "elite fitness 0.40040000000000003\n",
      "Generation:  120\n",
      "pop fitness 0.35776\n",
      "elite fitness 0.40040000000000003\n",
      "Generation:  121\n",
      "pop fitness 0.35968000000000006\n",
      "elite fitness 0.40040000000000003\n",
      "Generation:  122\n",
      "pop fitness 0.35191999999999996\n",
      "elite fitness 0.40040000000000003\n",
      "Generation:  123\n",
      "pop fitness 0.35603999999999997\n",
      "elite fitness 0.40040000000000003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation:  124\n",
      "pop fitness 0.35288\n",
      "elite fitness 0.40040000000000003\n",
      "Generation:  125\n",
      "pop fitness 0.36948000000000003\n",
      "elite fitness 0.40040000000000003\n",
      "Generation:  126\n",
      "pop fitness 0.366\n",
      "elite fitness 0.40040000000000003\n",
      "Generation:  127\n",
      "pop fitness 0.36644\n",
      "elite fitness 0.40040000000000003\n",
      "Generation:  128\n",
      "pop fitness 0.361\n",
      "elite fitness 0.40040000000000003\n",
      "Generation:  129\n",
      "pop fitness 0.3645999999999999\n",
      "elite fitness 0.40040000000000003\n",
      "Generation:  130\n",
      "pop fitness 0.34840000000000004\n",
      "elite fitness 0.40040000000000003\n",
      "Generation:  131\n",
      "pop fitness 0.35907999999999995\n",
      "elite fitness 0.40040000000000003\n",
      "Generation:  132\n",
      "pop fitness 0.34564\n",
      "elite fitness 0.40040000000000003\n",
      "Generation:  133\n",
      "pop fitness 0.35392\n",
      "elite fitness 0.40040000000000003\n",
      "Generation:  134\n",
      "pop fitness 0.35240000000000005\n",
      "elite fitness 0.40040000000000003\n",
      "Generation:  135\n",
      "pop fitness 0.3520399999999999\n",
      "elite fitness 0.40040000000000003\n",
      "Generation:  136\n",
      "pop fitness 0.35603999999999997\n",
      "elite fitness 0.40040000000000003\n",
      "Generation:  137\n",
      "pop fitness 0.3572800000000001\n",
      "elite fitness 0.4008000000000001\n",
      "Generation:  138\n",
      "pop fitness 0.3657599999999999\n",
      "elite fitness 0.4008000000000001\n",
      "Generation:  139\n",
      "pop fitness 0.35971999999999993\n",
      "elite fitness 0.4008000000000001\n",
      "Generation:  140\n",
      "pop fitness 0.36912\n",
      "elite fitness 0.4008000000000001\n",
      "Generation:  141\n",
      "pop fitness 0.3680799999999999\n",
      "elite fitness 0.4008000000000001\n",
      "Generation:  142\n",
      "pop fitness 0.38160000000000005\n",
      "elite fitness 0.4016\n",
      "Generation:  143\n",
      "pop fitness 0.36956000000000006\n",
      "elite fitness 0.4016\n",
      "Generation:  144\n",
      "pop fitness 0.3702\n",
      "elite fitness 0.4016\n",
      "Generation:  145\n",
      "pop fitness 0.36016000000000004\n",
      "elite fitness 0.4016\n",
      "Generation:  146\n",
      "pop fitness 0.35863999999999996\n",
      "elite fitness 0.402\n",
      "Generation:  147\n",
      "pop fitness 0.3582000000000001\n",
      "elite fitness 0.402\n",
      "Generation:  148\n",
      "pop fitness 0.36296000000000006\n",
      "elite fitness 0.402\n",
      "Generation:  149\n",
      "pop fitness 0.36096000000000006\n",
      "elite fitness 0.402\n",
      "Generation:  150\n",
      "pop fitness 0.3562400000000001\n",
      "elite fitness 0.402\n",
      "Generation:  151\n",
      "pop fitness 0.35996\n",
      "elite fitness 0.402\n",
      "Generation:  152\n",
      "pop fitness 0.36219999999999997\n",
      "elite fitness 0.40279999999999994\n",
      "Generation:  153\n",
      "pop fitness 0.36784\n",
      "elite fitness 0.4032\n",
      "Generation:  154\n",
      "pop fitness 0.36540000000000006\n",
      "elite fitness 0.4032\n",
      "Generation:  155\n",
      "pop fitness 0.35804\n",
      "elite fitness 0.4032\n",
      "Generation:  156\n",
      "pop fitness 0.35916000000000003\n",
      "elite fitness 0.4032\n",
      "Generation:  157\n",
      "pop fitness 0.36324\n",
      "elite fitness 0.4032\n",
      "Generation:  158\n",
      "pop fitness 0.3597200000000001\n",
      "elite fitness 0.4032\n",
      "Generation:  159\n",
      "pop fitness 0.36260000000000003\n",
      "elite fitness 0.4032\n",
      "Generation:  160\n",
      "pop fitness 0.36951999999999996\n",
      "elite fitness 0.4032\n",
      "Generation:  161\n",
      "pop fitness 0.37091999999999997\n",
      "elite fitness 0.4032\n",
      "Generation:  162\n",
      "pop fitness 0.36840000000000006\n",
      "elite fitness 0.4032\n",
      "Generation:  163\n",
      "pop fitness 0.36455999999999994\n",
      "elite fitness 0.4032\n",
      "Generation:  164\n",
      "pop fitness 0.36884\n",
      "elite fitness 0.4032\n",
      "Generation:  165\n",
      "pop fitness 0.35784\n",
      "elite fitness 0.4032\n",
      "Generation:  166\n",
      "pop fitness 0.36032000000000003\n",
      "elite fitness 0.41679999999999995\n",
      "Generation:  167\n",
      "pop fitness 0.36491999999999997\n",
      "elite fitness 0.41679999999999995\n",
      "Generation:  168\n",
      "pop fitness 0.36412000000000005\n",
      "elite fitness 0.41679999999999995\n",
      "Generation:  169\n",
      "pop fitness 0.3598799999999999\n",
      "elite fitness 0.41679999999999995\n",
      "Generation:  170\n",
      "pop fitness 0.36224000000000006\n",
      "elite fitness 0.41679999999999995\n",
      "Generation:  171\n",
      "pop fitness 0.35596000000000005\n",
      "elite fitness 0.41679999999999995\n",
      "Generation:  172\n",
      "pop fitness 0.36711999999999995\n",
      "elite fitness 0.41679999999999995\n",
      "Generation:  173\n",
      "pop fitness 0.37091999999999997\n",
      "elite fitness 0.42560000000000003\n",
      "Generation:  174\n",
      "pop fitness 0.36507999999999996\n",
      "elite fitness 0.42560000000000003\n",
      "Generation:  175\n",
      "pop fitness 0.36168\n",
      "elite fitness 0.42560000000000003\n",
      "Generation:  176\n",
      "pop fitness 0.36924\n",
      "elite fitness 0.42639999999999995\n",
      "Generation:  177\n",
      "pop fitness 0.37024\n",
      "elite fitness 0.42639999999999995\n",
      "Generation:  178\n",
      "pop fitness 0.37144000000000005\n",
      "elite fitness 0.42639999999999995\n",
      "Generation:  179\n",
      "pop fitness 0.3680800000000001\n",
      "elite fitness 0.42639999999999995\n",
      "Generation:  180\n",
      "pop fitness 0.36248\n",
      "elite fitness 0.4276\n",
      "Generation:  181\n",
      "pop fitness 0.36716\n",
      "elite fitness 0.4276\n",
      "Generation:  182\n",
      "pop fitness 0.3548400000000001\n",
      "elite fitness 0.4276\n",
      "Generation:  183\n",
      "pop fitness 0.3569199999999999\n",
      "elite fitness 0.4276\n",
      "Generation:  184\n",
      "pop fitness 0.36184\n",
      "elite fitness 0.42799999999999994\n",
      "Generation:  185\n",
      "pop fitness 0.36364000000000013\n",
      "elite fitness 0.43\n",
      "Generation:  186\n",
      "pop fitness 0.36052\n",
      "elite fitness 0.43\n",
      "Generation:  187\n",
      "pop fitness 0.36048\n",
      "elite fitness 0.43\n",
      "Generation:  188\n",
      "pop fitness 0.36003999999999997\n",
      "elite fitness 0.43\n",
      "Generation:  189\n",
      "pop fitness 0.36508\n",
      "elite fitness 0.43\n",
      "Generation:  190\n",
      "pop fitness 0.36168\n",
      "elite fitness 0.43\n",
      "Generation:  191\n",
      "pop fitness 0.3683600000000001\n",
      "elite fitness 0.44559999999999994\n",
      "Generation:  192\n",
      "pop fitness 0.36808\n",
      "elite fitness 0.44559999999999994\n",
      "Generation:  193\n",
      "pop fitness 0.36832\n",
      "elite fitness 0.44559999999999994\n",
      "Generation:  194\n",
      "pop fitness 0.37107999999999997\n",
      "elite fitness 0.44559999999999994\n",
      "Generation:  195\n",
      "pop fitness 0.36772000000000005\n",
      "elite fitness 0.44559999999999994\n",
      "Generation:  196\n",
      "pop fitness 0.36396\n",
      "elite fitness 0.4619999999999999\n",
      "Generation:  197\n",
      "pop fitness 0.35904\n",
      "elite fitness 0.4619999999999999\n",
      "Generation:  198\n",
      "pop fitness 0.37256\n",
      "elite fitness 0.4619999999999999\n",
      "Generation:  199\n",
      "pop fitness 0.36891999999999997\n",
      "elite fitness 0.466\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "from functools import reduce\n",
    "\n",
    "# population size\n",
    "psize = 50\n",
    "#mutation magnitude\n",
    "mag = 0.1\n",
    "# number of generations\n",
    "generations = 200\n",
    "#number of arms\n",
    "no_arms = 10\n",
    "\n",
    "#Number of samples for each bandit problem\n",
    "trials = 50\n",
    "\n",
    "greedy_kill=5\n",
    "\n",
    "\n",
    "#setting the enviroment\n",
    "envs  = [Enviroment(is_bernoulli=True, no_arms=no_arms) for _ in range(10)]\n",
    "\n",
    "#set of elite agent\n",
    "elite = [None]*5\n",
    "\n",
    "#counter\n",
    "k=0\n",
    "\n",
    "# Average rewards for ES\n",
    "avg_elite_rwd = 0\n",
    "avg_rwd = 0\n",
    "rwds = []\n",
    "elite_rwds = []\n",
    "\n",
    "#initilizing population\n",
    "population = []\n",
    "for _ in range(psize):\n",
    "    population.append(Individual(no_arms=no_arms, mag=mag, trials=trials))\n",
    "    population[-1].run(envs)\n",
    "  \n",
    "    \n",
    "for gen in range(generations):\n",
    "    \n",
    "    #Getting elite agent\n",
    "    values = [[copy.deepcopy(ind), ind.fitness] for ind in population]\n",
    "    values.sort(key = operator.itemgetter(1))\n",
    "    for x in enumerate(values[-(1+len(elite)):-1]):\n",
    "        elite[x[0]] =  x[1][0]\n",
    "    \n",
    "            \n",
    "    rwds.append(np.mean([ind.fitness for ind in population]))\n",
    "    elite_rwds.append(np.mean([ind.fitness for ind in elite]))\n",
    "    print('Generation: ', gen)\n",
    "    print('pop fitness', np.mean([ind.fitness for ind in population]))\n",
    "    print('elite fitness', np.mean([ind.fitness for ind in elite]))\n",
    "    \n",
    "    # Finding mean parameters of elite agent\n",
    "    mu = np.mean([ind.model.extract_parameters() for ind in elite], axis=0)\n",
    "    \n",
    "    children = [ind.mutate(mu) for ind in population]\n",
    "    [child.run(envs) for child in children]\n",
    "    \n",
    "    #Killing\n",
    "    all_individuals = population + children\n",
    "    for _ in range(psize):\n",
    "        to_kill = random.sample(all_individuals, greedy_kill)\n",
    "        to_kill = reduce(lambda x, y: x if x.fitness < y.fitness else y, to_kill)\n",
    "        all_individuals.remove(to_kill)\n",
    "        \n",
    "    population = all_individuals\n",
    "\n",
    "  #  print('test fitness', np.mean([ind.fitness for ind in population]))\n",
    "  #  [ind.run(env) for ind in elite]\n",
    "   # print('test_elite fitness', np.mean([ind.fitness for ind in elite]))\n",
    "\n",
    "'''\n",
    "#for agent in elite:\n",
    "#    print(agent.model(Variable(torch.Tensor(np.array([agent.info[arm]['no_visited'] for arm in range(no_arms)])))))\n",
    "'''\n",
    "\n",
    "#setting UCB agent\n",
    "ucb = AgentUCB(no_arms)\n",
    "\n",
    "#Average reward of UCB agent\n",
    "ucb_rwd = 0\n",
    "for env in envs:\n",
    "    for _ in range(trials):    \n",
    "        arm = ucb.pull()\n",
    "        r = env.pull_arm(arm)\n",
    "        ucb_rwd += int(env.best_arm==arm)\n",
    "        ucb.update(arm, r)\n",
    "    ucb.reset()\n",
    "\n",
    "ucb_rwd /= len(envs)*trials\n",
    "        \n",
    "        \n",
    "\n",
    "plt.plot(range(len(rwds)), rwds)\n",
    "plt.plot(range(len(rwds)), elite_rwds)\n",
    "plt.plot(range(len(rwds)), [ucb_rwd]*len(rwds))\n",
    "\n",
    "\n",
    "\n",
    "plt.ylabel('Returns')\n",
    "plt.xlabel('Generations') \n",
    "plt.legend(['pop_avg_return', 'elite_avg_return', 'UCB return'], loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Pop return   elite_return\n",
      "              0            0.1           0.12\n",
      "              1           0.11           0.13\n",
      "              2           0.11           0.13\n",
      "              3           0.12           0.14\n",
      "              4           0.12           0.14\n",
      "              5           0.13           0.15\n",
      "              6           0.13           0.15\n",
      "              7           0.14           0.15\n",
      "              8           0.14           0.16\n",
      "              9           0.15           0.18\n",
      "             10           0.17           0.19\n",
      "             11           0.18            0.2\n",
      "             12           0.19           0.22\n",
      "             13            0.2           0.22\n",
      "             14           0.21           0.23\n",
      "             15           0.21           0.24\n",
      "             16           0.21           0.25\n",
      "             17           0.21           0.25\n",
      "             18           0.22           0.25\n",
      "             19           0.22           0.25\n",
      "             20           0.22           0.26\n",
      "             21           0.23           0.28\n",
      "             22           0.23           0.28\n",
      "             23           0.23           0.28\n",
      "             24           0.24           0.28\n",
      "             25           0.24           0.29\n",
      "             26           0.24           0.31\n",
      "             27           0.25           0.31\n",
      "             28           0.25           0.31\n",
      "             29           0.25           0.31\n",
      "             30           0.25           0.32\n",
      "             31           0.25           0.32\n",
      "             32           0.26           0.32\n",
      "             33           0.25           0.32\n",
      "             34           0.26           0.33\n",
      "             35           0.27           0.33\n",
      "             36           0.27           0.34\n",
      "             37           0.28           0.36\n",
      "             38           0.28           0.36\n",
      "             39           0.28           0.36\n",
      "             40           0.28           0.36\n",
      "             41           0.28           0.36\n",
      "             42           0.29           0.37\n",
      "             43            0.3           0.37\n",
      "             44           0.29           0.37\n",
      "             45            0.3           0.37\n",
      "             46            0.3           0.37\n",
      "             47            0.3           0.38\n",
      "             48           0.31           0.38\n",
      "             49           0.31           0.38\n",
      "             50           0.32           0.38\n",
      "             51           0.32           0.38\n",
      "             52           0.32           0.38\n",
      "             53           0.32           0.38\n",
      "             54           0.32           0.38\n",
      "             55           0.32           0.38\n",
      "             56           0.32           0.38\n",
      "             57           0.32           0.38\n",
      "             58           0.33           0.38\n",
      "             59           0.32           0.38\n",
      "             60           0.33           0.38\n",
      "             61           0.32           0.38\n",
      "             62           0.33           0.38\n",
      "             63           0.32           0.38\n",
      "             64           0.32           0.38\n",
      "             65           0.33           0.38\n",
      "             66           0.32           0.38\n",
      "             67           0.32           0.38\n",
      "             68           0.32           0.38\n",
      "             69           0.33           0.38\n",
      "             70           0.33           0.38\n",
      "             71           0.32           0.38\n",
      "             72           0.33           0.38\n",
      "             73           0.33           0.38\n",
      "             74           0.32           0.38\n",
      "             75           0.32           0.38\n",
      "             76           0.32           0.38\n",
      "             77           0.32           0.38\n",
      "             78           0.34           0.38\n",
      "             79           0.32           0.38\n",
      "             80           0.32           0.38\n",
      "             81           0.33           0.38\n",
      "             82           0.33           0.38\n",
      "             83           0.33           0.38\n",
      "             84           0.33           0.39\n",
      "             85           0.34           0.39\n",
      "             86           0.33           0.39\n",
      "             87           0.33           0.39\n",
      "             88           0.33           0.39\n",
      "             89           0.34           0.39\n",
      "             90           0.34           0.39\n",
      "             91           0.34           0.39\n",
      "             92           0.35           0.39\n",
      "             93           0.35           0.39\n",
      "             94           0.35           0.39\n",
      "             95           0.36           0.39\n",
      "             96           0.34           0.39\n",
      "             97           0.34            0.4\n",
      "             98           0.35            0.4\n",
      "             99           0.34            0.4\n",
      "            100           0.35            0.4\n",
      "            101           0.35            0.4\n",
      "            102           0.36            0.4\n",
      "            103           0.35            0.4\n",
      "            104           0.36            0.4\n",
      "            105           0.35            0.4\n",
      "            106           0.36            0.4\n",
      "            107           0.35            0.4\n",
      "            108           0.36            0.4\n",
      "            109           0.35            0.4\n",
      "            110           0.36            0.4\n",
      "            111           0.36            0.4\n",
      "            112           0.35            0.4\n",
      "            113           0.36            0.4\n",
      "            114           0.35            0.4\n",
      "            115           0.36            0.4\n",
      "            116           0.35            0.4\n",
      "            117           0.36            0.4\n",
      "            118           0.36            0.4\n",
      "            119           0.35            0.4\n",
      "            120           0.36            0.4\n",
      "            121           0.36            0.4\n",
      "            122           0.35            0.4\n",
      "            123           0.36            0.4\n",
      "            124           0.35            0.4\n",
      "            125           0.37            0.4\n",
      "            126           0.37            0.4\n",
      "            127           0.37            0.4\n",
      "            128           0.36            0.4\n",
      "            129           0.36            0.4\n",
      "            130           0.35            0.4\n",
      "            131           0.36            0.4\n",
      "            132           0.35            0.4\n",
      "            133           0.35            0.4\n",
      "            134           0.35            0.4\n",
      "            135           0.35            0.4\n",
      "            136           0.36            0.4\n",
      "            137           0.36            0.4\n",
      "            138           0.37            0.4\n",
      "            139           0.36            0.4\n",
      "            140           0.37            0.4\n",
      "            141           0.37            0.4\n",
      "            142           0.38            0.4\n",
      "            143           0.37            0.4\n",
      "            144           0.37            0.4\n",
      "            145           0.36            0.4\n",
      "            146           0.36            0.4\n",
      "            147           0.36            0.4\n",
      "            148           0.36            0.4\n",
      "            149           0.36            0.4\n",
      "            150           0.36            0.4\n",
      "            151           0.36            0.4\n",
      "            152           0.36            0.4\n",
      "            153           0.37            0.4\n",
      "            154           0.37            0.4\n",
      "            155           0.36            0.4\n",
      "            156           0.36            0.4\n",
      "            157           0.36            0.4\n",
      "            158           0.36            0.4\n",
      "            159           0.36            0.4\n",
      "            160           0.37            0.4\n",
      "            161           0.37            0.4\n",
      "            162           0.37            0.4\n",
      "            163           0.36            0.4\n",
      "            164           0.37            0.4\n",
      "            165           0.36            0.4\n",
      "            166           0.36           0.42\n",
      "            167           0.36           0.42\n",
      "            168           0.36           0.42\n",
      "            169           0.36           0.42\n",
      "            170           0.36           0.42\n",
      "            171           0.36           0.42\n",
      "            172           0.37           0.42\n",
      "            173           0.37           0.43\n",
      "            174           0.37           0.43\n",
      "            175           0.36           0.43\n",
      "            176           0.37           0.43\n",
      "            177           0.37           0.43\n",
      "            178           0.37           0.43\n",
      "            179           0.37           0.43\n",
      "            180           0.36           0.43\n",
      "            181           0.37           0.43\n",
      "            182           0.35           0.43\n",
      "            183           0.36           0.43\n",
      "            184           0.36           0.43\n",
      "            185           0.36           0.43\n",
      "            186           0.36           0.43\n",
      "            187           0.36           0.43\n",
      "            188           0.36           0.43\n",
      "            189           0.37           0.43\n",
      "            190           0.36           0.43\n",
      "            191           0.37           0.45\n",
      "            192           0.37           0.45\n",
      "            193           0.37           0.45\n",
      "            194           0.37           0.45\n",
      "            195           0.37           0.45\n",
      "            196           0.36           0.46\n",
      "            197           0.36           0.46\n",
      "            198           0.37           0.46\n",
      "            199           0.37           0.47\n"
     ]
    }
   ],
   "source": [
    "data = np.array([[round(a, 2) ,round(b, 2)] for a,b in zip(rwds, elite_rwds)])\n",
    "teams_list = ['Pop return', 'elite_return']\n",
    "row_format =\"{:>15}\" * (len(teams_list) + 1)\n",
    "print(row_format.format(\"\", *teams_list))\n",
    "for team, row in zip(range(len(rwds)), data):\n",
    "    print(row_format.format(team, *row))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_envs  = [Enviroment(is_bernoulli=True, no_arms=no_arms) for _ in range(10)]\n",
    "test_pop = []\n",
    "test_elite = []\n",
    "test_ucb = []\n",
    "for env in test_envs:\n",
    "    test_rwd = 0\n",
    "    for indv in population:\n",
    "        indv.played = False\n",
    "        indv.score = 0\n",
    "        indv.fitness = 0\n",
    "        indv.run([env])\n",
    "        test_rwd += indv.fitness\n",
    "    test_pop.append(round(test_rwd/50, 3))\n",
    "    \n",
    "    test_rwd = 0\n",
    "    for indv in elite:\n",
    "        indv.played = False\n",
    "        indv.score = 0\n",
    "        indv.fitness = 0\n",
    "        indv.run([env])\n",
    "        test_rwd += indv.fitness\n",
    "    test_elite.append(round(test_rwd/50, 3))\n",
    "    \n",
    "    test_rwd = 0\n",
    "    for _ in range(trials):    \n",
    "        arm = ucb.pull()\n",
    "        r = env.pull_arm(arm)\n",
    "        test_rwd += int(env.best_arm==arm)\n",
    "        ucb.update(arm, r)\n",
    "    ucb.reset()\n",
    "    test_ucb.append(round(test_rwd/50, 3))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Env ' + str(i) for i in range(len(test_envs))]\n",
    "\n",
    "x = np.arange(len(labels))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width/2, test_pop, width/2, label='Population')\n",
    "rects2 = ax.bar(x , test_elite, width/2, label='Elite')\n",
    "rects3 = ax.bar(x + width/2, test_ucb, width/2, label='UCB')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Scores')\n",
    "ax.set_title('Scores for different envirnoments')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "def autolabel(rects):\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate('{}'.format(height),\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "\n",
    "autolabel(rects1)\n",
    "autolabel(rects2)\n",
    "autolabel(rects3)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
